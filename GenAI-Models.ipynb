{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.8.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision==0.23.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.8.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting torchtext==0.18.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchtext-0.18.0%2Bcpu-cp312-cp312-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting filelock (from torch==2.8.0+cpu)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0+cpu) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0+cpu) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.8.0+cpu)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.8.0+cpu)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.8.0+cpu) (3.1.5)\n",
      "Collecting fsspec (from torch==2.8.0+cpu)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy (from torchvision==0.23.0+cpu)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.23.0+cpu)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0+cpu) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torchtext==0.18.0+cpu) (2.32.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.8.0+cpu)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.8.0+cpu) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0+cpu) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0+cpu) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0+cpu) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torchtext==0.18.0+cpu) (2024.12.14)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (183.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchvision-0.23.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.8.0%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, fsspec, filelock, torch, torchvision, torchtext, torchaudio\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.12.0 mpmath-1.3.0 networkx-3.6.1 numpy-2.3.5 pillow-12.0.0 sympy-1.14.0 torch-2.8.0+cpu torchaudio-2.8.0+cpu torchtext-0.18.0+cpu torchvision-0.23.0+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers==4.42.1\n",
      "  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting numpy==1.26\n",
      "  Downloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.42.1)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.42.1)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.1)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.42.1)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers==4.42.1) (4.67.1)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.13.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (2025.12.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers==4.42.1) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-4.42.1-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m265.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.4-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, sentencepiece, safetensors, regex, protobuf, optree, opt_einsum, numpy, mdurl, markdown, hf-xet, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, ml_dtypes, markdown-it-py, huggingface-hub, h5py, tokenizers, rich, transformers, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.5\n",
      "    Uninstalling numpy-2.3.5:\n",
      "      Successfully uninstalled numpy-2.3.5\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 hf-xet-1.2.0 huggingface-hub-0.36.0 keras-3.13.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 numpy-1.26.0 opt_einsum-3.4.0 optree-0.18.0 protobuf-6.33.2 regex-2025.11.3 rich-14.2.0 safetensors-0.7.0 sentencepiece-0.2.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 tokenizers-0.19.1 transformers-4.42.1 werkzeug-3.1.4 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu torchtext==0.18.0+cpu  \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers==4.42.1 tensorflow sentencepiece numpy==1.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Importing the required tools from the transformers library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90acd6173d5f46e1a832706873d9b3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53ca6cc9c154f70ae648c927e9a5958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/730M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8b8462a99c4601a0fee0ce446ff1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87886c7146b437bb4905d7cf860e3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b1a1d71f8645fba14b611f11e5f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbba24c4ca740f9b0f6a42d92a87a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c94f0d900148e6abf98bd88b185480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7f3427790745f381f380b09bcab1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f40717aa4849e390c396a4a142adcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Selecting the model. You will be using \"facebook/blenderbot-400M-distill\" in this example.\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the initialization, let's set up the chat function to enable real-time interaction with the chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! How are you? I just got back from walking my dog. Do you have any pets?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Define the chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Trying another language model and comparing the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9222b7aa6f2d49d09b64b231e5c5caa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d842bc8e3644d7899da993c0c467af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5e5cbe16414faaae2c5925299559c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f637526317b34558b857a1122871f397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565d306714954eff8e5429f8c12095ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b662fa7c8fba4cf8a74352e17c17d4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c70b23fe89e44d9b9371581360c0100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: i'm a sailor and i'm a sailor. i'm a sailor and i'm a sailor.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "### Let's chat with another bot\n",
    "def chat_with_another_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_another_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many language models available in Hugging Face. In the following exercise, you will compare the output for the same input using two different models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a chatbot using different models from Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple chatbot using the transformers library from Hugging Face(https://huggingface.co/models). Run the code using the following model and compare the output. The model is \"[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\". \n",
    "**(Note: Based on the selected model, you may notice differences in the chatbot output. Multiple factors, such as model training and fine-tuning, influence the output.)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63767d2a88644c679babea5478bf31c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b6777528404842b142d9cfa2dc1207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d4882537aa4b1995133dc304b76c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2005f74e2164d7ebc4de0dc88300598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b4437932de46889ba1ccfa2e41f9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f89bacdc1741ce8074f5601ade71f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a351af0d1bf407bb9f4573de5f56a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add code for the exercise here:\n",
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-small\" #here the model name can be changed as you like.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "import sentencepiece\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-small\" #here the model name can be changed as you like.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Downloading and Saving Chatbot Models\n",
      "============================================================\n",
      "\n",
      "This may take several minutes depending on your internet speed.\n",
      "Total download size: ~2-3 GB\n",
      "\n",
      "Downloading facebook/blenderbot-400M-distill...\n",
      "Downloading tokenizer...\n",
      "Downloading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 60, 'min_length': 20, 'num_beams': 10, 'length_penalty': 0.65, 'no_repeat_ngram_size': 3, 'encoder_no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved to ./models/blenderbot\n",
      "\n",
      "Downloading google/flan-t5-base...\n",
      "Downloading tokenizer...\n",
      "Downloading model...\n",
      "âœ“ Saved to ./models/flan-t5-base\n",
      "\n",
      "Downloading google/flan-t5-small...\n",
      "Downloading tokenizer...\n",
      "Downloading model...\n",
      "âœ“ Saved to ./models/flan-t5-small\n",
      "\n",
      "============================================================\n",
      "All models downloaded successfully!\n",
      "You can now run the Streamlit app with: streamlit run chatbot_app.py\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "def save_model(model_name, save_dir):\n",
    "    \"\"\"Download and save a model and its tokenizer\"\"\"\n",
    "    print(f\"\\nDownloading {model_name}...\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Download and save tokenizer\n",
    "    print(\"Downloading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    \n",
    "    # Download and save model\n",
    "    print(\"Downloading model...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    model.save_pretrained(save_dir)\n",
    "    \n",
    "    print(f\"âœ“ Saved to {save_dir}\")\n",
    "\n",
    "def main():\n",
    "    # Define models to download\n",
    "    models = {\n",
    "        \"BlenderBot\": {\n",
    "            \"name\": \"facebook/blenderbot-400M-distill\",\n",
    "            \"save_dir\": \"./models/blenderbot\"\n",
    "        },\n",
    "        \"FLAN-T5 Base\": {\n",
    "            \"name\": \"google/flan-t5-base\",\n",
    "            \"save_dir\": \"./models/flan-t5-base\"\n",
    "        },\n",
    "        \"FLAN-T5 Small\": {\n",
    "            \"name\": \"google/flan-t5-small\",\n",
    "            \"save_dir\": \"./models/flan-t5-small\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Downloading and Saving Chatbot Models\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThis may take several minutes depending on your internet speed.\")\n",
    "    print(\"Total download size: ~2-3 GB\")\n",
    "    \n",
    "    # Download each model\n",
    "    for display_name, model_info in models.items():\n",
    "        try:\n",
    "            save_model(model_info[\"name\"], model_info[\"save_dir\"])\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error downloading {display_name}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All models downloaded successfully!\")\n",
    "    print(\"You can now run the Streamlit app with: streamlit run chatbot_app.py\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ Initializing Multi-Model Chatbot Interface...\n",
      "======================================================================\n",
      "\n",
      "âœ¨ Interface will appear below. Follow these steps:\n",
      "1ï¸âƒ£  Select a model from the dropdown\n",
      "2ï¸âƒ£  Click 'Load Model' button\n",
      "3ï¸âƒ£  Wait for loading to complete\n",
      "4ï¸âƒ£  Start chatting!\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f545c8d2109403da13b512d19f3e9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value=\"\\n            <div style='background: linear-gradient(135deg, #667eeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Model Chatbot - Interactive Notebook Version\n",
    "Works directly in JupyterLab without networking issues\n",
    "\"\"\"\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Suppress tokenizer warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Install ipywidgets if needed\n",
    "try:\n",
    "    import ipywidgets\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    print(\"Installing ipywidgets...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ipywidgets\", \"-q\"])\n",
    "    import ipywidgets as widgets\n",
    "    print(\"âœ… Installed successfully!\")\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"BlenderBot (Conversational)\": {\n",
    "        \"path\": \"./models/blenderbot\",\n",
    "        \"description\": \"Facebook's conversational chatbot - Best for casual conversation\"\n",
    "    },\n",
    "    \"FLAN-T5 Base (General Purpose)\": {\n",
    "        \"path\": \"./models/flan-t5-base\",\n",
    "        \"description\": \"Google's FLAN-T5 base model - Good for general tasks\"\n",
    "    },\n",
    "    \"FLAN-T5 Small (Lightweight)\": {\n",
    "        \"path\": \"./models/flan-t5-small\",\n",
    "        \"description\": \"Smaller FLAN-T5 model - Fastest responses\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class ChatbotApp:\n",
    "    def __init__(self):\n",
    "        self.current_model = None\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.chat_history = []\n",
    "        self.create_ui()\n",
    "        \n",
    "    def create_ui(self):\n",
    "        \"\"\"Create the user interface\"\"\"\n",
    "        \n",
    "        # Header\n",
    "        self.header = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "                        padding: 25px; border-radius: 15px; margin-bottom: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);'>\n",
    "                <h1 style='color: white; margin: 0; text-align: center; font-size: 2.5em;'>\n",
    "                    ğŸ¤– Multi-Model AI Chatbot\n",
    "                </h1>\n",
    "                <p style='color: white; margin: 10px 0 0 0; text-align: center; font-size: 1.2em;'>\n",
    "                    Choose a model and start chatting!\n",
    "                </p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Model selector\n",
    "        self.model_dropdown = widgets.Dropdown(\n",
    "            options=list(MODELS.keys()),\n",
    "            description='AI Model:',\n",
    "            style={'description_width': '120px'},\n",
    "            layout=widgets.Layout(width='600px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Model description\n",
    "        self.model_info = widgets.HTML(\n",
    "            value=f\"<p style='color: #666; font-style: italic; font-size: 1.1em; margin: 10px 0;'>{MODELS[self.model_dropdown.value]['description']}</p>\"\n",
    "        )\n",
    "        \n",
    "        # Load model button\n",
    "        self.load_button = widgets.Button(\n",
    "            description='ğŸš€ Load Model',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='200px', height='45px'),\n",
    "            style={'font_weight': 'bold'}\n",
    "        )\n",
    "        \n",
    "        # Status message\n",
    "        self.status = widgets.HTML(\n",
    "            value=\"<p style='color: #999; font-size: 1.1em;'>ğŸ‘† Select a model and click Load Model to start</p>\"\n",
    "        )\n",
    "        \n",
    "        # Chat display area\n",
    "        self.chat_output = widgets.Output(\n",
    "            layout=widgets.Layout(\n",
    "                height='500px',\n",
    "                border='3px solid #667eea',\n",
    "                border_radius='15px',\n",
    "                padding='20px',\n",
    "                overflow_y='auto',\n",
    "                background_color='#f5f7fa'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # User input\n",
    "        self.user_input = widgets.Textarea(\n",
    "            placeholder='ğŸ’¬ Type your message here... (Shift+Enter for new line)',\n",
    "            layout=widgets.Layout(width='100%', height='100px', border='2px solid #ddd', border_radius='10px', padding='10px'),\n",
    "            disabled=True,\n",
    "            style={'font_size': '14px'}\n",
    "        )\n",
    "        \n",
    "        # Button container\n",
    "        self.send_button = widgets.Button(\n",
    "            description='ğŸ“¤ Send Message',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='180px', height='50px'),\n",
    "            disabled=True,\n",
    "            style={'font_weight': 'bold'}\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='ğŸ—‘ï¸ Clear Chat',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='180px', height='50px'),\n",
    "            style={'font_weight': 'bold'}\n",
    "        )\n",
    "        \n",
    "        # Max tokens slider\n",
    "        self.max_tokens = widgets.IntSlider(\n",
    "            value=150,\n",
    "            min=50,\n",
    "            max=300,\n",
    "            step=10,\n",
    "            description='Response Length:',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='500px'),\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        # Message counter\n",
    "        self.msg_counter = widgets.HTML(value=\"<p style='font-size: 1.1em; color: #666;'>ğŸ“Š Messages: 0</p>\")\n",
    "        \n",
    "        # Event handlers\n",
    "        self.model_dropdown.observe(self.on_model_change, 'value')\n",
    "        self.load_button.on_click(self.load_model)\n",
    "        self.send_button.on_click(self.send_message)\n",
    "        self.clear_button.on_click(self.clear_chat)\n",
    "        \n",
    "    def on_model_change(self, change):\n",
    "        model_name = change['new']\n",
    "        self.model_info.value = f\"<p style='color: #666; font-style: italic; font-size: 1.1em; margin: 10px 0;'>{MODELS[model_name]['description']}</p>\"\n",
    "        \n",
    "    def load_model(self, button):\n",
    "        model_name = self.model_dropdown.value\n",
    "        \n",
    "        self.status.value = f\"<p style='color: orange; font-size: 1.1em;'>â³ Loading {model_name}... Please wait...</p>\"\n",
    "        self.load_button.disabled = True\n",
    "        self.load_button.description = \"â³ Loading...\"\n",
    "        \n",
    "        try:\n",
    "            model_path = MODELS[model_name][\"path\"]\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "            \n",
    "            self.current_model = model_name\n",
    "            self.status.value = f\"<p style='color: green; font-size: 1.2em; font-weight: bold;'>âœ… {model_name} loaded successfully!</p>\"\n",
    "            \n",
    "            self.user_input.disabled = False\n",
    "            self.send_button.disabled = False\n",
    "            self.load_button.disabled = False\n",
    "            self.load_button.description = \"ğŸš€ Load Model\"\n",
    "            \n",
    "            self.add_message(\"assistant\", f\"Hello! I'm {model_name.split('(')[0].strip()}. How can I help you today?\", model_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status.value = f\"<p style='color: red; font-size: 1.1em;'>âŒ Error: {str(e)}</p>\"\n",
    "            self.load_button.disabled = False\n",
    "            self.load_button.description = \"ğŸš€ Load Model\"\n",
    "    \n",
    "    def generate_response(self, user_text):\n",
    "        try:\n",
    "            inputs = self.tokenizer.encode(user_text, return_tensors=\"pt\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=self.max_tokens.value,\n",
    "                    num_beams=4,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def add_message(self, role, content, model_name=None):\n",
    "        timestamp = datetime.now().strftime(\"%I:%M %p\")\n",
    "        \n",
    "        if role == \"user\":\n",
    "            message_html = f\"\"\"\n",
    "            <div style='margin: 15px 0; text-align: right; animation: slideIn 0.3s;'>\n",
    "                <div style='display: inline-block; max-width: 75%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "                            color: white; padding: 15px 20px; border-radius: 20px 20px 5px 20px;\n",
    "                            box-shadow: 0 3px 10px rgba(102, 126, 234, 0.3); font-size: 1.05em;'>\n",
    "                    <strong style='font-size: 1.1em;'>ğŸ‘¤ You</strong> \n",
    "                    <span style='font-size: 0.85em; opacity: 0.9; margin-left: 8px;'>{timestamp}</span>\n",
    "                    <p style='margin: 8px 0 0 0; line-height: 1.5;'>{content}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            model_tag = f\"<span style='font-size: 0.8em; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 3px 10px; border-radius: 12px; margin-left: 8px;'>{model_name.split('(')[0].strip()}</span>\" if model_name else \"\"\n",
    "            message_html = f\"\"\"\n",
    "            <div style='margin: 15px 0; text-align: left; animation: slideIn 0.3s;'>\n",
    "                <div style='display: inline-block; max-width: 75%; background: white; \n",
    "                            padding: 15px 20px; border-radius: 20px 20px 20px 5px;\n",
    "                            box-shadow: 0 3px 10px rgba(0,0,0,0.1); border: 2px solid #e8e8e8; font-size: 1.05em;'>\n",
    "                    <strong style='font-size: 1.1em;'>ğŸ¤– AI</strong> {model_tag}\n",
    "                    <span style='font-size: 0.85em; color: #999; margin-left: 8px;'>{timestamp}</span>\n",
    "                    <p style='margin: 8px 0 0 0; color: #333; line-height: 1.6;'>{content}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        with self.chat_output:\n",
    "            display(HTML(message_html))\n",
    "        \n",
    "        self.chat_history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"model\": model_name,\n",
    "            \"timestamp\": timestamp\n",
    "        })\n",
    "        \n",
    "        self.msg_counter.value = f\"<p style='font-size: 1.1em; color: #666;'>ğŸ“Š Messages: {len(self.chat_history)}</p>\"\n",
    "    \n",
    "    def send_message(self, button):\n",
    "        user_text = self.user_input.value.strip()\n",
    "        \n",
    "        if not user_text:\n",
    "            return\n",
    "        \n",
    "        self.add_message(\"user\", user_text)\n",
    "        self.user_input.value = \"\"\n",
    "        \n",
    "        self.send_button.disabled = True\n",
    "        self.send_button.description = \"ğŸ¤” Thinking...\"\n",
    "        \n",
    "        response = self.generate_response(user_text)\n",
    "        self.add_message(\"assistant\", response, self.current_model)\n",
    "        \n",
    "        self.send_button.disabled = False\n",
    "        self.send_button.description = \"ğŸ“¤ Send Message\"\n",
    "    \n",
    "    def clear_chat(self, button):\n",
    "        self.chat_output.clear_output()\n",
    "        self.chat_history = []\n",
    "        self.msg_counter.value = \"<p style='font-size: 1.1em; color: #666;'>ğŸ“Š Messages: 0</p>\"\n",
    "        \n",
    "        if self.current_model:\n",
    "            self.add_message(\"assistant\", f\"Chat cleared! Ready for a fresh conversation.\", self.current_model)\n",
    "    \n",
    "    def display(self):\n",
    "        # Main layout\n",
    "        model_section = widgets.VBox([\n",
    "            self.header,\n",
    "            widgets.HBox([self.model_dropdown, self.load_button], layout=widgets.Layout(margin='15px 0', justify_content='center')),\n",
    "            self.model_info,\n",
    "            self.status\n",
    "        ])\n",
    "        \n",
    "        chat_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h2 style='margin: 30px 0 15px 0; color: #333;'>ğŸ’¬ Conversation</h2>\"),\n",
    "            self.chat_output,\n",
    "            widgets.HTML(\"<div style='margin: 15px 0;'></div>\"),\n",
    "            self.user_input,\n",
    "            widgets.HBox([self.send_button, self.clear_button, self.msg_counter], \n",
    "                        layout=widgets.Layout(margin='15px 0', justify_content='space-between'))\n",
    "        ])\n",
    "        \n",
    "        settings_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3 style='margin: 25px 0 15px 0; color: #333;'>âš™ï¸ Settings</h3>\"),\n",
    "            self.max_tokens,\n",
    "            widgets.HTML(\"\"\"\n",
    "                <div style='margin-top: 20px; padding: 15px; background: #f0f0f0; border-radius: 10px;'>\n",
    "                    <p style='margin: 0; color: #666;'><strong>ğŸ’¡ Tips:</strong></p>\n",
    "                    <ul style='margin: 10px 0; color: #666;'>\n",
    "                        <li><strong>BlenderBot</strong>: Best for natural conversations</li>\n",
    "                        <li><strong>FLAN-T5 Base</strong>: Great for questions & tasks</li>\n",
    "                        <li><strong>FLAN-T5 Small</strong>: Fastest response times</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        main_layout = widgets.VBox([\n",
    "            model_section,\n",
    "            chat_section,\n",
    "            settings_section\n",
    "        ], layout=widgets.Layout(padding='30px', max_width='1000px', margin='0 auto'))\n",
    "        \n",
    "        display(main_layout)\n",
    "\n",
    "# Launch the chatbot\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸš€ Initializing Multi-Model Chatbot Interface...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nâœ¨ Interface will appear below. Follow these steps:\")\n",
    "print(\"1ï¸âƒ£  Select a model from the dropdown\")\n",
    "print(\"2ï¸âƒ£  Click 'Load Model' button\")\n",
    "print(\"3ï¸âƒ£  Wait for loading to complete\")\n",
    "print(\"4ï¸âƒ£  Start chatting!\")\n",
    "print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "chatbot = ChatbotApp()\n",
    "chatbot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description|}\n",
    "```\n",
    "```{|-|-|-|-|}\n",
    "```\n",
    "```{|2023-09-10|0.1|Vicky Kuo|Initial Lab Created|}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "7f514257e51d2341694edbcac1009cee4c5546b62460691e3e52ef8f664a9021"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
